{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Building a Real-Time Analytics Service with InfluxDB\n",
    "\n",
    "Jeremiah Malina  \n",
    "\n",
    "Slides and code can be found at http://github.com/jjmalina/pygotham-2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* I'm a data engineer at [![](http://chatid.com/images/img_logo_chatid.png)](http://chatid.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Our Product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Allows brands such as Samsung, HP, Seagate, etc. to plug their existing chat system (LivePerson, Olark, etc.) into the ChatID platform via XMPP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* So that customers on retailers like Walmart.com, Sears.com, Newegg.com can chat with the folks at these brands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* We collect a lot of data to measure the impact that our product has on ecommerce metrics like conversion rate, average order value, customer satisfaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* [Here's a product demo on a mock retailer site](https://demo.chatid.com/chatbar/retailer/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview of this talk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1 - Real-Time Analytics\n",
    "  * Exploring a problem\n",
    "  * Paradigms\n",
    "  * Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "2 - Dive into InfluxDB\n",
    "  * Features\n",
    "  * Modeling data\n",
    "  * Issues to watch out for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "3 - Building our Real-Time Analytics Service\n",
    "  * live coding :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Real-Time Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### If you manage a website you might find yourself asking \n",
    "\n",
    "> How many came users to my site from referral campaign Z in the last X minutes/hours/days/months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Let's assume that we don't have Google Analytics or a similar tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Some approaches to tackling this problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1 - Write down every visit to your site with the referral campaign ID into a database, and then run a query to get a metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "2 - Feed the visit events as a stream into a process that computes the metrics in real-time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Problems with #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* You have to scale your database to handle the write throughput\n",
    "  * i.e. disk, memory, CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* You need to index the referral campaign ID and probably other fields, which slows write throughput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Problems with #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* You need a counter for every combination of fields and time periods you want to index by (i.e. campaign)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* You need to persist the counter state somewhere\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Parallelizing the counting is a tricky problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* You need to build an API to fetch your counts and perhaps aggregate them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lambda Architecture\n",
    "\n",
    "Invented by Nathan Marz, the author of Storm  \n",
    "\n",
    "> The LA aims to satisfy the needs for a robust system that is fault-tolerant, both against hardware failures and human mistakes, being able to serve a wide range of workloads and use cases, and in which low-latency reads and updates are required. The resulting system should be linearly scalable, and it should scale out rather than up.\n",
    "\n",
    "http://lambda-architecture.net/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](http://lambda-architecture.net/img/la-overview_small.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1 - All data entering the system is dispatched to both the batch layer and the speed layer for processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "2 - The batch layer has two functions:\n",
    "  * managing the master dataset (an immutable, append-only set of raw data),\n",
    "  * to pre-compute the batch views."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "3 - The serving layer indexes the batch views so that they can be queried in low-latency, ad-hoc way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "4 - The speed layer compensates for the high latency of updates to the serving layer and deals with recent data only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "5 - Any incoming query can be answered by merging results from batch views and real-time view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### That's a lot of components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://pbs.twimg.com/media/BNELF1GCUAExynU.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### There's this idea that real-time layer is more prone to failure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Reasons\n",
    "\n",
    "* The computations happen in memory\n",
    "* Anything that leads to a crash will erase the state of the real-time system\n",
    "  * Bad deploy\n",
    "  * Out of memory\n",
    "  * Delayed upstream data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The batch layer is there to correct this\n",
    "\n",
    "> Since the realtime layer only compensates for the last few hours of data, everything the realtime layer computes is eventually overridden by the batch layer. So if you make a mistake or something goes wrong in the realtime layer, the batch layer will correct it.\n",
    "\n",
    "-- Nathan Marz, [**Big Data**](http://www.amazon.com/Big-Data-Principles-practices-scalable/dp/1617290343/ref=sr_1_1?ie=UTF8&qid=1439167413&sr=8-1&keywords=nathan+marz+big+data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### But component #1 of the Lambda Architecture doesn't really get addressed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### In comes Jay Kreps with \"The Log\"\n",
    "\n",
    "[The Log: What every software engineer should know about real-time data's unifying abstraction](https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Turn this\n",
    "![](https://engineering.linkedin.com/sites/default/files/datapipeline_complex.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Into this\n",
    "![](https://engineering.linkedin.com/sites/default/files/datapipeline_simple.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Lambda Architecture Stack\n",
    "![](http://lambda-architecture.net/img/la-overview_small.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1. Unified Log\n",
    "\n",
    "* Apache Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2. Batch Layer\n",
    "\n",
    "* Hadoop for storage\n",
    "* MapReduce or Spark for computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3. Serving layer\n",
    "\n",
    "* Cassandra, or any other KV, SQL-like datastore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 4. Real-time layer\n",
    "\n",
    "* Apache Storm or Spark Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Unfortunately all run on the JVM and Python support isn't the best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Real-Time Computation Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Apache Storm\n",
    "\n",
    ">  Storm makes it easy to reliably process unbounded streams of data, doing for realtime processing what Hadoop did for batch processing.\n",
    "\n",
    "https://storm.apache.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are just three abstractions in Storm: **spouts**, **bolts**, and **topologies**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A **spout** is a source of streams in a computation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A **bolt** processes any number of input streams and produces any number of new output streams.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A **topology** is a network of spouts and bolts, with each edge in the network representing a bolt subscribing to the output stream of some other **spout** or **bolt**.\n",
    "\n",
    "![](https://storm.apache.org/images/topology.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Using Streamparse\n",
    "\n",
    "https://github.com/Parsely/streamparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "from streamparse.bolt import BatchingBolt\n",
    "\n",
    "\n",
    "class VistCounterBolt(BatchingBolt):\n",
    "    def initialize(self, conf, ctx):\n",
    "        self.counts = Counter()\n",
    "\n",
    "    def group_key(self, tup):\n",
    "        \"\"\"Groups user visit tuples by campaign and minute\"\"\"\n",
    "        timestamp, campaign = tup.values[1].split('\\t')\n",
    "        hour = datetime.utcfromtimestamp(float(timestamp)).strftime(\"%Y-%m-%dT%H:%M\")\n",
    "        return (hour, campaign)\n",
    "\n",
    "    def process_batch(self, key, tups):\n",
    "        self.counts[key] += len(tups)\n",
    "        \n",
    "        # some downstream bolt would persist our counts\n",
    "        self.emit([key, self.counts[key]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Storm Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Good\n",
    "\n",
    "* Simple but flexible programming model\n",
    "  * Thank you Parse.ly!\n",
    "* Scalable and fault tolerant\n",
    "* Supports many inputs\n",
    "* Very stable and mature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Bad\n",
    "\n",
    "* No built in libraries or APIs for analytical computation\n",
    "  * i.e. grouping, counting, distinct counting\n",
    "* Still need another service to store your metrics\n",
    "* In Streamparse you have to set up topologies with Clojure\n",
    "* [\"Easy to deploy\"](https://storm.apache.org/documentation/Setting-up-a-Storm-cluster.html) despite needing a ZooKeeper cluster\n",
    "* [\"Productionizing Storm is Difficult\"](http://www.alexgallego.org/storm/streaming/production/operations/2015/02/22/storm-missmatched-expectations.html)\n",
    "  * tl;dr; wanted to consume from a different Kafka node, had to restart the whole topology, there is no graceful transition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Spark Streaming\n",
    "\n",
    "> Spark Streaming makes it easy to build scalable fault-tolerant streaming applications.\n",
    "\n",
    "http://spark.apache.org/streaming/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> Spark Streaming provides a high-level abstraction called discretized stream or DStream, which represents a continuous stream of data. DStreams can be created either from input data streams from sources such as Kafka, Flume, and Kinesis, or by applying high-level operations on other DStreams. Internally, a DStream is represented as a sequence of RDDs.\n",
    "\n",
    "![](http://spark.apache.org/docs/latest/img/streaming-flow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a local StreamingContext with two working thread and batch interval of 1 second\n",
    "sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "# Assume we have our visit events coming in from a socket\n",
    "events = ssc.socketTextStream(\"localhost\", 1337)\n",
    "\n",
    "\n",
    "def process_event(data):\n",
    "    ts, campaign = data.split('\\t')\n",
    "    minute = datetime.utcfromtimetamp(float(ts)).strftime(\"%Y-%m-%dT%H:%M\")\n",
    "    return minute + campaign, 1\n",
    "\n",
    "\n",
    "def persist_count(key, count):\n",
    "    \"\"\"Take an hourly count and persist it\n",
    "    \"\"\"\n",
    "    print(key, count)\n",
    "\n",
    "    \n",
    "events \\\n",
    "    \n",
    "    # parse the data from the socket\n",
    "    .map(process_event) \\ \n",
    "\n",
    "    # group events by their key every 60 seconds in a window\n",
    "    .groupByKeyAndWindow(windowDuration=60, slideDuration=60) \\\n",
    "    .foreachRDD(\n",
    "        # count the number of events per key, then persist them\n",
    "        lambda rdd: rdd.countByValue().map(persist_count)  \n",
    "    )\n",
    "\n",
    "ssc.start()             # Start the computation\n",
    "ssc.awaitTermination()  # Wait for the computation to terminate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Good\n",
    "\n",
    "* Python works out of the box\n",
    "* Programming model is more simple than Storm\n",
    "** no topology, just chain together your functions\n",
    "* Built-in APIs for analytical computations, i.e count, group by, reduce\n",
    "* Fault tolerant and scalable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Bad\n",
    "\n",
    "* Python doesn't have access to all APIs  \n",
    "  * Only sockets, files, or Kafka are supported inputs\n",
    "* The pyspark library isn't available as a package so you have to separate it from code you want to test outside Spark\n",
    "* Data is processed in a batch interval, so there's more latency than Storm\n",
    "* Deploying is more simple than Storm but still might require ZooKeeper, or you can use YARN or Apache Mesos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Back to our problem\n",
    "\n",
    "1. Write down every visit to your site with the referral campaign ID into a database, and then run a query to get a metric\n",
    "2. Feed the visit events as a stream into a process that computes the metrics in real-time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Option 1 seems more simple "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Let's say you just want real-time metrics for the past 24 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why not just go for PostgreSQL and intelligently index?\n",
    "\n",
    "* Shard data into tables by day and periodically drop the old tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### InfluxDB actually works like this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# InfluxDB\n",
    "\n",
    "> InfluxDB is a time series, metrics, and analytics database. It’s written in Go and has no external dependencies. That means once you install it there’s nothing else to manage (such as Redis, ZooKeeper, Cassandra, HBase, or anything else).\n",
    "\n",
    "https://influxdb.com/docs/v0.9/introduction/overview.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Installation is simple\n",
    "\n",
    "On a Mac: \n",
    "\n",
    "```\n",
    "$ brew update && brew install influxdb\n",
    "$ influxd\n",
    "```\n",
    "\n",
    "On Ubuntu or Debian Linux:\n",
    "\n",
    "```\n",
    "$ wget http://influxdb.s3.amazonaws.com/influxdb_0.9.2_amd64.deb\n",
    "$ sudo dpkg -i influxdb_0.9.2_amd64.deb\n",
    "$ sudo /etc/init.d/influxdb start\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Writing data\n",
    "\n",
    "```\n",
    "curl -G http://localhost:8086/query --data-urlencode \"q=CREATE DATABASE realtime_analytics\"\n",
    "curl -i -XPOST 'http://localhost:8086/write?db=realtime_analytics' --data-binary 'visits,url=\"/index.html\",campaign_id=\"12345 user_id=\"7b5a96d3-a9d8-469a-91aa-a95f869d350c\" 1439258675695870000'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Line Protocol\n",
    "\n",
    "`<measurement>[,<tag-key>=<tag-value>...] <field-key>=<field-value>[,<field2-key>=<field2-value>...] [unix-nano-timestamp]`\n",
    "\n",
    "**Measurements** are like SQL tables  \n",
    "**Tags** are indexes on data points  \n",
    "**Fields** are like columns, which aren't indexed  \n",
    "**Timestamps** can be arbitrary precision but the default is nanoseconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Series and Tags\n",
    "\n",
    "> A series is defined as a combination of a measurement and set of tag key-values. Combined with fields (columns) and the fields’ values, these make up series data.\n",
    "\n",
    "Let's say you write `visits,url=\"/index.html\",campaign_id=\"12345 value=0.64 1422568543702900257`\n",
    "\n",
    "Then you'll end up with a four series\n",
    "\n",
    "1. visits\n",
    "2. vists + url=\"/index.html\"\n",
    "3. visits + campaign_id=\"12345\"\n",
    "4. visits + url=\"/index.html\" + campaign_id=\"12345\"\n",
    "\n",
    "Each with a data point where user_id=\"7b5a96d3-a9d8-469a-91aa-a95f869d350c\" at time 1422568543702900257"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Be aware of the cardinality of your series\n",
    "\n",
    "InfluxDB series are stored in memory by the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Querying\n",
    "\n",
    "```\n",
    "SELECT count(distinct(user_id)) from visits where url='/index.html' and campaign_id='12345';\n",
    "```\n",
    "\n",
    "Since `visits + url=\"/index.html\" + campaign_id=\"12345\"` is its own series this query doesn't have to do a scan to find these values.\n",
    "\n",
    "Summarize all visits under a campaign for all URLs: `SELECT count(distinct(user_id)) from visits where campaign_id='12345';`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How do we handle an influx of data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Batch your events into fewer HTTP requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Retention Policies\n",
    "\n",
    "```\n",
    "ALTER RETENTION POLICY default ON realtime_analytics DURATION 1d DEFAULT;\n",
    "```\n",
    "\n",
    "* Now InfluxDB will by default automatically drop data in database `realtime_analytics` older than two days\n",
    "* Data is removed efficiently because it is sharded by time\n",
    "  * So a delete is not equivalent to each write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Wait but I still want my historical metrics!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Continuous Queries\n",
    "\n",
    "1. Create a new retention policy: `CREATE RETENTION POLICY historical_metrics ON realtime_analytics DURATION INF REPLICATION 1;`\n",
    "2. Add a continuous query\n",
    "\n",
    "```\n",
    "CREATE CONTINUOUS QUERY historical_metrics_count_impressions_1d ON realtime_analytics BEGIN\n",
    "  SELECT COUNT(DISTINCT(user_id)) INTO realtime_analytics.historical_metrics.impressions_count_1d FROM visits GROUP BY time(1d), *\n",
    "END\n",
    "```\n",
    "\n",
    "Now InfluxDB will run a query once a day to write into a downsamples series of unique visitor counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### We have a mini Lambda Architecture!\n",
    "![](http://lambda-architecture.net/img/la-overview_small.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Caveats of Continuous Queries and Retention Policies\n",
    "\n",
    "1. [The docs aren't the clearest here](https://influxdb.com/docs/v0.9/query_language/continuous_queries.html)\n",
    "2. There is a [better explanation on the mailing list](https://groups.google.com/forum/?utm_medium=email&utm_source=footer#!msg/influxdb/ThjSbqYEdFw/_up8O23UtdsJ)\n",
    "3. You can't backfill historical data yet. See [issue 211](https://github.com/influxdb/influxdb/issues/211)\n",
    "4. If you need to window your daily counts by different timezones then it's better to sample your metrics by hour\n",
    "  * but that breaks the accuracy of distinct counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## InfluxDB Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Good\n",
    "\n",
    "* Easy to deploy\n",
    "* Simple HTTP API\n",
    "* Writes are fast and can be batched\n",
    "* Data is indexed by tags and time\n",
    "* [Measurement and tag discovery API](https://influxdb.com/docs/v0.9/query_language/schema_exploration.html) which is great for dashboards\n",
    "* Efficient deletes, retention policies and downsampling continuous queries\n",
    "* SQL\n",
    "* Visualization with [Grafana](http://grafana.org/) or [Chronograf](https://influxdb.com/chronograf/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Bad\n",
    "\n",
    "* Some issues with queries, i.e. can't have expressions inside aggregation functions\n",
    "* Continuous queries and retention policies have some [issues](https://github.com/influxdb/influxdb/issues/3368) and limitations\n",
    "  * expensive CQs can cause writes to time out\n",
    "* Clustering has limitations (3 node max currently)\n",
    "* [Time series data is a hard problem for distributed systems](http://www.thedotpost.com/2015/06/paul-dix-time-series-data-the-worst-and-best-use-case-in-distributed-databases)\n",
    "  * From version 0.8.x to 0.9.x InfluxDB has basically been rewritten and uses a different storage engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Our Real-Time Analytics Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A fun example is the Coinbase order & trades feed\n",
    "\n",
    "https://docs.exchange.coinbase.com/?python#websocket-feed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Consuming it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import websockets\n",
    "\n",
    "COINBASE_FEED_URL = \"wss://ws-feed.exchange.coinbase.com\"\n",
    "\n",
    "\n",
    "@asyncio.coroutine\n",
    "def coinbase_feed():\n",
    "    websocket = yield from websockets.connect(COINBASE_FEED_URL)\n",
    "    yield from websocket.send(json.dumps({\n",
    "        \"type\": \"subscribe\",\n",
    "        \"product_id\": \"BTC-USD\"\n",
    "    }))\n",
    "    while True:\n",
    "        message = yield from websocket.recv()\n",
    "        print(message)\n",
    "\n",
    "\n",
    "def main():\n",
    "    asyncio.get_event_loop().run_until_complete(coinbase_feed())\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### There are two types of orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**limit**: have a price and size, and are filled at the price specified or better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**market**: have funds, and or size, and are fulfilled immediately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Both types have a `side` which is either `buy` or `sell`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Market orders appear to happen infrequently so we're going to ignore them for this project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Limit orders have types\n",
    "\n",
    "* **received**: the order has been received by Coinbase\n",
    "* **open**: the order is active in the order book\n",
    "* **done**: the order is no longer active, because it was either filled partially/fully or canceled\n",
    "* **match**: there was a match between a buy order and sell order which resulted in a trade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Received order\n",
    "\n",
    "\n",
    "```\n",
    "{\n",
    "  \"funds\": null,\n",
    "  \"time\": \"2015-08-06T23:28:29.815115Z\",\n",
    "  \"order_type\": \"limit\",\n",
    "  \"order_id\": \"6b81fa22-bf1a-46e3-bed8-c989af7cbbd7\",\n",
    "  \"size\": \"0.438\",\n",
    "  \"product_id\": \"BTC-USD\",\n",
    "  \"price\": \"279.91\",\n",
    "  \"type\": \"received\",\n",
    "  \"side\": \"sell\",\n",
    "  \"sequence\": 171951254\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Open order\n",
    "\n",
    "```\n",
    "{\n",
    "  \"time\": \"2015-08-06T23:28:29.815246Z\",\n",
    "  \"product_id\": \"BTC-USD\",\n",
    "  \"remaining_size\": \"0.438\",\n",
    "  \"order_id\": \"6b81fa22-bf1a-46e3-bed8-c989af7cbbd7\",\n",
    "  \"price\": \"279.91\",\n",
    "  \"type\": \"open\",\n",
    "  \"side\": \"sell\",\n",
    "  \"sequence\": 171951255\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Done order  \n",
    "\n",
    "\n",
    "```\n",
    "{\n",
    "   \"order_type\" : \"limit\",\n",
    "   \"sequence\" : 171991560,\n",
    "   \"time\" : \"2015-08-07T00:31:03.487784Z\",\n",
    "   \"product_id\" : \"BTC-USD\",\n",
    "   \"type\" : \"done\",\n",
    "   \"reason\" : \"canceled\",\n",
    "   \"side\" : \"buy\",\n",
    "   \"remaining_size\" : \"8.29\",\n",
    "   \"order_id\" : \"18d4b572-2d07-48b4-b086-d119d37585c8\",\n",
    "   \"price\" : \"279.52\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Match orders\n",
    "\n",
    "```\n",
    "{\n",
    "   \"sequence\" : 171988199,\n",
    "   \"size\" : \"0.1236\",\n",
    "   \"side\" : \"sell\",\n",
    "   \"maker_order_id\" : \"627708c9-ca29-4d14-b7b4-886ee282de52\",\n",
    "   \"time\" : \"2015-08-07T00:22:25.317866Z\",\n",
    "   \"trade_id\" : 3101496,\n",
    "   \"type\" : \"match\",\n",
    "   \"price\" : \"280.01\",\n",
    "   \"taker_order_id\" : \"8edc7b0e-dbd7-4cfc-9b5d-65b26b136c89\",\n",
    "   \"product_id\" : \"BTC-USD\"\n",
    "}\n",
    "{\n",
    "   \"sequence\" : 171988199,\n",
    "   \"price\" : \"280.01\",\n",
    "   \"type\" : \"match\",\n",
    "   \"taker_order_id\" : \"8edc7b0e-dbd7-4cfc-9b5d-65b26b136c89\",\n",
    "   \"maker_order_id\" : \"627708c9-ca29-4d14-b7b4-886ee282de52\",\n",
    "   \"trade_id\" : 3101496,\n",
    "   \"time\" : \"2015-08-07T00:22:25.317866Z\",\n",
    "   \"side\" : \"buy\",\n",
    "   \"size\" : \"0.1236\",\n",
    "   \"product_id\" : \"BTC-USD\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Real-time Coinbase metrics we're interested in\n",
    "\n",
    "* Average order buy/sell price\n",
    "* Average order buy/sell size\n",
    "* Average trade price\n",
    "* Average trade size \n",
    "* Volume: the amount of BTC that's been traded\n",
    "* Total $ traded\n",
    "* High buy/sell price\n",
    "* Low buy/sell price\n",
    "* Average spread - the difference between the average buy and average sell price\n",
    "  * the smaller the spread the more demand for BTC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Modeling orders in InfluxDB\n",
    "\n",
    "```\n",
    "{\n",
    "  \"time\": \"2015-08-06T23:28:29.815246Z\",\n",
    "  \"product_id\": \"BTC-USD\",\n",
    "  \"remaining_size\": \"0.438\",\n",
    "  \"order_id\": \"6b81fa22-bf1a-46e3-bed8-c989af7cbbd7\",\n",
    "  \"price\": \"279.91\",\n",
    "  \"type\": \"open\",\n",
    "  \"side\": \"sell\",\n",
    "  \"sequence\": 171951255\n",
    "}\n",
    "```\n",
    "\n",
    "Becomes:  \n",
    "\n",
    "`orders,side=\"sell\",type=\"open\" price=279.91,size=0.438 1438903709000000000`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Modeling trades in InfluxDB  \n",
    "\n",
    "```\n",
    "{\n",
    "   \"sequence\" : 171988199,\n",
    "   \"price\" : \"280.01\",\n",
    "   \"type\" : \"match\",\n",
    "   \"taker_order_id\" : \"8edc7b0e-dbd7-4cfc-9b5d-65b26b136c89\",\n",
    "   \"maker_order_id\" : \"627708c9-ca29-4d14-b7b4-886ee282de52\",\n",
    "   \"trade_id\" : 3101496,\n",
    "   \"time\" : \"2015-08-07T00:22:25.317866Z\",\n",
    "   \"side\" : \"sell\",\n",
    "   \"size\" : \"0.1236\",\n",
    "   \"product_id\" : \"BTC-USD\"\n",
    "}\n",
    "```\n",
    "\n",
    "Becomes:  \n",
    "\n",
    "`trades,side=\"sell\",type=\"uptick\" price=280.01,size=0.1236,cost=34.609236 1438906945000000000`  \n",
    "\n",
    "Since the side was \"sell\", we'll say the type is an uptick because the buyer met the seller's price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### OK let's write some data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Thank You\n",
    "\n",
    "* PyGotham\n",
    "* ChatID\n",
    "* InfluxDB team & contributors\n",
    "* Everyone on the next slide :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Works Cited\n",
    "\n",
    "* Marz, Nathan, and James Warren. Big Data Principles and Best Practices of Scalable Real - Time Data Systems. Shelter Island, NY: Manning Publ., 2015\n",
    "* Hausenblas, Michael, and Nathan Bijnens. \"Lambda Architecture\". N.p., 2015. http://lambda-architecture.net \n",
    "* Kreps, Jay. \"The Log: What Every Software Engineer Should Know about Real-time Data's Unifying Abstraction.\". Web. 16 Dec. 2013. <https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying>.\n",
    "* \"Storm, Distributed and Fault-tolerant Realtime Computation.\" Storm, Distributed and Fault-tolerant Realtime Computation. Apache Software Foundation, n.d. Web. <https://storm.apache.org/>.\n",
    "* Streamparse. <https://github.com/Parsely/streamparse>.\n",
    "* Gallego, Alexander. \"Productionizing Storm Is Difficult.\" <http://www.alexgallego.org/storm/streaming/production/operations/2015/02/22/storm-missmatched-expectations.html>.\n",
    "* Spark Streaming. Apache Software Foundation. <http://spark.apache.org/streaming/>.\n",
    "* InfluxDB. InfluxDB. <https://influxdb.com/docs/v0.9/introduction/overview.html>.\n",
    "* Dix, Paul. \"Time Series Data: TheP Worst and Best Use Case in Distributed Databases.\" Time Series Data: The Best and Worst Use Case in Distributed Databases. The Dot Post, 8 June 2015. Web. <http://www.thedotpost.com/2015/06/paul-dix-time-series-data-the-worst-and-best-use-case-in-distributed-databases>.\n",
    "* Coinbase | Exchange API Reference. Coinbase, <https://docs.exchange.coinbase.com/?python#websocket-feed>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Questions? Comments?\n",
    "\n",
    "Ask away!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
